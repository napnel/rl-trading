{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import cql, Trainer, pg\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": \"Pendulum-v1\",\n",
    "    \"framework\": \"torch\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOS_PATH = Path.cwd().parent / \"videos\"\n",
    "EXPERIMENT_PATH = Path.cwd().parent / \"experience\"\n",
    "VIDEOS_PATH.mkdir(exist_ok=True, parents=True)\n",
    "EXPERIMENT_PATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(env: gym.Env, agent = None, show_video = False):\n",
    "    env = RecordVideo(env, VIDEOS_PATH / config[\"env\"])\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_steps = 1 \n",
    "    while not done:\n",
    "        if agent:\n",
    "            action = agent.compute_single_action(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "    if show_video:\n",
    "        from IPython.display import Video\n",
    "        video_path = list((VIDEOS_PATH / config[\"env\"]).glob(\"*.mp4\"))[0]\n",
    "        return Video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 200\n",
      "counters:\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_env_steps_sampled: 200\n",
      "  num_env_steps_trained: 200\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_16-03-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1567108044175\n",
      "episode_reward_mean: -878.1567108044175\n",
      "episode_reward_min: -878.1567108044175\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 1a1ab261c6fd422eb88a2a97d8787eef\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -358.562744140625\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_env_steps_sampled: 200\n",
      "  num_env_steps_trained: 200\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.29.40.230\n",
      "num_agent_steps_sampled: 200\n",
      "num_agent_steps_trained: 200\n",
      "num_env_steps_sampled: 200\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 200\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 5.7\n",
      "  gpu_util_percent0: 0.02\n",
      "  ram_util_percent: 18.6\n",
      "  vram_util_percent0: 0.177490234375\n",
      "pid: 15435\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0815237339456283\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.10636552649350901\n",
      "  mean_inference_ms: 0.7083427846728272\n",
      "  mean_raw_obs_processing_ms: 0.08401586048638642\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1567108044175\n",
      "  episode_reward_mean: -878.1567108044175\n",
      "  episode_reward_min: -878.1567108044175\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -878.1567108044175\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0815237339456283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10636552649350901\n",
      "    mean_inference_ms: 0.7083427846728272\n",
      "    mean_raw_obs_processing_ms: 0.08401586048638642\n",
      "time_since_restore: 0.2109696865081787\n",
      "time_this_iter_s: 0.2109696865081787\n",
      "time_total_s: 0.2109696865081787\n",
      "timers:\n",
      "  learn_throughput: 23072.883\n",
      "  learn_time_ms: 8.668\n",
      "  load_throughput: 866591.736\n",
      "  load_time_ms: 0.231\n",
      "timestamp: 1655967801\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 400\n",
      "counters:\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_env_steps_sampled: 400\n",
      "  num_env_steps_trained: 400\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_16-03-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1567108044175\n",
      "episode_reward_mean: -1056.2177132254358\n",
      "episode_reward_min: -1234.278715646454\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 2\n",
      "experiment_id: 1a1ab261c6fd422eb88a2a97d8787eef\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -467.78240966796875\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_env_steps_sampled: 400\n",
      "  num_env_steps_trained: 400\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.29.40.230\n",
      "num_agent_steps_sampled: 400\n",
      "num_agent_steps_trained: 400\n",
      "num_env_steps_sampled: 400\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 400\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf: {}\n",
      "pid: 15435\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07678685481905548\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.10060873728641731\n",
      "  mean_inference_ms: 0.6620657994042238\n",
      "  mean_raw_obs_processing_ms: 0.07709083769395716\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1567108044175\n",
      "  episode_reward_mean: -1056.2177132254358\n",
      "  episode_reward_min: -1234.278715646454\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -878.1567108044175\n",
      "    - -1234.278715646454\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07678685481905548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10060873728641731\n",
      "    mean_inference_ms: 0.6620657994042238\n",
      "    mean_raw_obs_processing_ms: 0.07709083769395716\n",
      "time_since_restore: 0.3658335208892822\n",
      "time_this_iter_s: 0.15486383438110352\n",
      "time_total_s: 0.3658335208892822\n",
      "timers:\n",
      "  learn_throughput: 26734.469\n",
      "  learn_time_ms: 7.481\n",
      "  load_throughput: 622531.206\n",
      "  load_time_ms: 0.321\n",
      "timestamp: 1655967801\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 600\n",
      "counters:\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_env_steps_sampled: 600\n",
      "  num_env_steps_trained: 600\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_16-03-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1567108044175\n",
      "episode_reward_mean: -1094.7249254924952\n",
      "episode_reward_min: -1234.278715646454\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 3\n",
      "experiment_id: 1a1ab261c6fd422eb88a2a97d8787eef\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -459.3670349121094\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_env_steps_sampled: 600\n",
      "  num_env_steps_trained: 600\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.29.40.230\n",
      "num_agent_steps_sampled: 600\n",
      "num_agent_steps_trained: 600\n",
      "num_env_steps_sampled: 600\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 600\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf: {}\n",
      "pid: 15435\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07673614097879392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.10006695173754852\n",
      "  mean_inference_ms: 0.6567084501410627\n",
      "  mean_raw_obs_processing_ms: 0.07513802850260243\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1567108044175\n",
      "  episode_reward_mean: -1094.7249254924952\n",
      "  episode_reward_min: -1234.278715646454\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -878.1567108044175\n",
      "    - -1234.278715646454\n",
      "    - -1171.7393500266141\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07673614097879392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10006695173754852\n",
      "    mean_inference_ms: 0.6567084501410627\n",
      "    mean_raw_obs_processing_ms: 0.07513802850260243\n",
      "time_since_restore: 0.5690696239471436\n",
      "time_this_iter_s: 0.20323610305786133\n",
      "time_total_s: 0.5690696239471436\n",
      "timers:\n",
      "  learn_throughput: 28982.534\n",
      "  learn_time_ms: 6.901\n",
      "  load_throughput: 693464.425\n",
      "  load_time_ms: 0.288\n",
      "timestamp: 1655967801\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 800\n",
      "counters:\n",
      "  num_agent_steps_sampled: 800\n",
      "  num_agent_steps_trained: 800\n",
      "  num_env_steps_sampled: 800\n",
      "  num_env_steps_trained: 800\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_16-03-21\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1567108044175\n",
      "episode_reward_mean: -1061.6357120749976\n",
      "episode_reward_min: -1234.278715646454\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 4\n",
      "experiment_id: 1a1ab261c6fd422eb88a2a97d8787eef\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -382.001220703125\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 800\n",
      "  num_agent_steps_trained: 800\n",
      "  num_env_steps_sampled: 800\n",
      "  num_env_steps_trained: 800\n",
      "iterations_since_restore: 4\n",
      "node_ip: 172.29.40.230\n",
      "num_agent_steps_sampled: 800\n",
      "num_agent_steps_trained: 800\n",
      "num_env_steps_sampled: 800\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 800\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf: {}\n",
      "pid: 15435\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07680715926977594\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09997760549357157\n",
      "  mean_inference_ms: 0.6547200408070338\n",
      "  mean_raw_obs_processing_ms: 0.07413788053143279\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1567108044175\n",
      "  episode_reward_mean: -1061.6357120749976\n",
      "  episode_reward_min: -1234.278715646454\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -878.1567108044175\n",
      "    - -1234.278715646454\n",
      "    - -1171.7393500266141\n",
      "    - -962.3680718225053\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07680715926977594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09997760549357157\n",
      "    mean_inference_ms: 0.6547200408070338\n",
      "    mean_raw_obs_processing_ms: 0.07413788053143279\n",
      "time_since_restore: 0.7651576995849609\n",
      "time_this_iter_s: 0.19608807563781738\n",
      "time_total_s: 0.7651576995849609\n",
      "timers:\n",
      "  learn_throughput: 25274.885\n",
      "  learn_time_ms: 7.913\n",
      "  load_throughput: 659741.093\n",
      "  load_time_ms: 0.303\n",
      "timestamp: 1655967801\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 800\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 1000\n",
      "counters:\n",
      "  num_agent_steps_sampled: 1000\n",
      "  num_agent_steps_trained: 1000\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_trained: 1000\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_16-03-22\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -878.1567108044175\n",
      "episode_reward_mean: -1104.7556876529143\n",
      "episode_reward_min: -1277.2355899645806\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 5\n",
      "experiment_id: 1a1ab261c6fd422eb88a2a97d8787eef\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -565.1253662109375\n",
      "      model: {}\n",
      "  num_agent_steps_sampled: 1000\n",
      "  num_agent_steps_trained: 1000\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_trained: 1000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.29.40.230\n",
      "num_agent_steps_sampled: 1000\n",
      "num_agent_steps_trained: 1000\n",
      "num_env_steps_sampled: 1000\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 1000\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.7\n",
      "  gpu_util_percent0: 0.06\n",
      "  ram_util_percent: 18.6\n",
      "  vram_util_percent0: 0.1795654296875\n",
      "pid: 15435\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.07688477140405907\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.09983284863610917\n",
      "  mean_inference_ms: 0.6536159326595536\n",
      "  mean_raw_obs_processing_ms: 0.07335766034376374\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -878.1567108044175\n",
      "  episode_reward_mean: -1104.7556876529143\n",
      "  episode_reward_min: -1277.2355899645806\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -878.1567108044175\n",
      "    - -1234.278715646454\n",
      "    - -1171.7393500266141\n",
      "    - -962.3680718225053\n",
      "    - -1277.2355899645806\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07688477140405907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09983284863610917\n",
      "    mean_inference_ms: 0.6536159326595536\n",
      "    mean_raw_obs_processing_ms: 0.07335766034376374\n",
      "time_since_restore: 0.9542429447174072\n",
      "time_this_iter_s: 0.1890852451324463\n",
      "time_total_s: 0.9542429447174072\n",
      "timers:\n",
      "  learn_throughput: 25607.815\n",
      "  learn_time_ms: 7.81\n",
      "  load_throughput: 695803.583\n",
      "  load_time_ms: 0.287\n",
      "timestamp: 1655967802\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "def collect_experience(num_episodes):\n",
    "    config[\"output\"] = str(EXPERIMENT_PATH)\n",
    "    env = gym.make(config[\"env\"])\n",
    "    agent = pg.PGTrainer(config=config)\n",
    "    for i in range(num_episodes):\n",
    "        print(pretty_print(agent.train()))\n",
    "\n",
    "collect_experience(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/napnel/experience/output-2022-06-23_16-03-21_worker-0_0.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"input\"] = [str(path) for path in list(EXPERIMENT_PATH.glob(\"*.json\"))]\n",
    "config[\"output\"] = None\n",
    "config[\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_file=/home/napnel/experience exists=False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"data_file={} exists={}\".format(EXPERIMENT_PATH, os.path.isfile(EXPERIMENT_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 batches (1000 ts) into the replay buffer, which has capacity 1000000.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "step() needs to return a dict.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/napnel/rl-bot/offline_rl.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/napnel/rl-bot/offline_rl.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(config[\u001b[39m\"\u001b[39m\u001b[39menv\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/napnel/rl-bot/offline_rl.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/napnel/rl-bot/offline_rl.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m     results \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/napnel/rl-bot/offline_rl.ipynb#ch0000008vscode-remote?line=5'>6</a>\u001b[0m     episode \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mepisodes_total\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/napnel/rl-bot/offline_rl.ipynb#ch0000008vscode-remote?line=6'>7</a>\u001b[0m     step \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtimesteps_total\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml-bot/lib/python3.8/site-packages/ray/tune/trainable.py:316\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    315\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep()\n\u001b[0;32m--> 316\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mstep() needs to return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m \u001b[39m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m RESULT_DUPLICATE \u001b[39min\u001b[39;00m result:\n",
      "\u001b[0;31mAssertionError\u001b[0m: step() needs to return a dict."
     ]
    }
   ],
   "source": [
    "agent = cql.CQLTrainer(config=config, env=config[\"env\"])\n",
    "env = gym.make(config[\"env\"])\n",
    "\n",
    "for i in range(10):\n",
    "    results = agent.train()\n",
    "    episode = results.get(\"episodes_total\")\n",
    "    step = results.get(\"timesteps_total\")\n",
    "    reward = results.get(\"episode_reward_mean\")\n",
    "    print(f\"Iter {i} | Episode {episode} | Step {step} | Reward {reward}\")\n",
    "    if step >= 100000:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('ml-bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee554be5372d57a1a8d0c8789d456036f96c86b5a6259585cc7b77cce32bbd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
