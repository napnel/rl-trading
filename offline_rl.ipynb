{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import cql, Trainer, pg\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML, Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"env\": \"CartPole-v0\",\n",
    "    \"framework\": \"torch\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOS_PATH = Path.cwd() / \"videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 13:05:56,120\tWARNING deprecation.py:46 -- DeprecationWarning: `ray.rllib.agents.pg.default_config::DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.agents.pg.pg.PGConfig(...)` instead. This will raise an error in the future!\n",
      "/home/napnel/miniconda3/envs/ml-bot/lib/python3.8/site-packages/gym/wrappers/record_video.py:41: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/napnel/rl-bot/videos/CartPole-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/napnel/ray_results/PGTrainer_CartPole-v0_2022-06-19_13-05-569w212npi/\n"
     ]
    }
   ],
   "source": [
    "def evaluation(env: gym.Env, agent = None, show_video = False):\n",
    "    env = RecordVideo(env, VIDEOS_PATH / config[\"env\"])\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_steps = 1 \n",
    "    while not done:\n",
    "        if agent:\n",
    "            action = agent.compute_single_action(obs)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    env.close()\n",
    "\n",
    "    if show_video:\n",
    "        from IPython.display import Video\n",
    "        video_path = list((VIDEOS_PATH / config[\"env\"]).glob(\"*.mp4\"))[0]\n",
    "        return Video(video_path)\n",
    "\n",
    "agent = pg.PGTrainer(config=config)\n",
    "print(agent.logdir)\n",
    "env = gym.make(config[\"env\"])\n",
    "evaluation(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/napnel/rl-bot/videos/CartPole-v0/rl-video-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 11:54:41,864\tWARNING deprecation.py:46 -- DeprecationWarning: `ray.rllib.agents.pg.default_config::DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.agents.pg.pg.PGConfig(...)` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 200\n",
      "counters:\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_env_steps_sampled: 200\n",
      "  num_env_steps_trained: 200\n",
      "custom_metrics: {}\n",
      "date: 2022-06-19_11-54-42\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -200.0\n",
      "episode_reward_mean: -200.0\n",
      "episode_reward_min: -200.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 4618d3c50d314674b7d4a8c2235cbbb5\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -62.76473617553711\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 200.0\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_env_steps_sampled: 200\n",
      "  num_env_steps_trained: 200\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.20.53.128\n",
      "num_agent_steps_sampled: 200\n",
      "num_agent_steps_trained: 200\n",
      "num_env_steps_sampled: 200\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 200\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 0.8\n",
      "  gpu_util_percent0: 0.31\n",
      "  ram_util_percent: 11.7\n",
      "  vram_util_percent0: 0.099609375\n",
      "pid: 7313\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.042470533456375345\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.07504491663690822\n",
      "  mean_inference_ms: 0.5882023578852563\n",
      "  mean_raw_obs_processing_ms: 0.06580234166994617\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -200.0\n",
      "  episode_reward_mean: -200.0\n",
      "  episode_reward_min: -200.0\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -200.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042470533456375345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07504491663690822\n",
      "    mean_inference_ms: 0.5882023578852563\n",
      "    mean_raw_obs_processing_ms: 0.06580234166994617\n",
      "time_since_restore: 0.16974234580993652\n",
      "time_this_iter_s: 0.16974234580993652\n",
      "time_total_s: 0.16974234580993652\n",
      "timers:\n",
      "  learn_throughput: 19007.994\n",
      "  learn_time_ms: 10.522\n",
      "  load_throughput: 1047266.916\n",
      "  load_time_ms: 0.191\n",
      "  training_iteration_time_ms: 169.452\n",
      "  update_time_ms: 0.043\n",
      "timestamp: 1655607282\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 0.3437197208404541\n",
      "\n",
      "agent_timesteps_total: 400\n",
      "counters:\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_env_steps_sampled: 400\n",
      "  num_env_steps_trained: 400\n",
      "custom_metrics: {}\n",
      "date: 2022-06-19_11-54-42\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -200.0\n",
      "episode_reward_mean: -200.0\n",
      "episode_reward_min: -200.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 2\n",
      "experiment_id: 4618d3c50d314674b7d4a8c2235cbbb5\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -62.869041442871094\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 200.0\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_env_steps_sampled: 400\n",
      "  num_env_steps_trained: 400\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.20.53.128\n",
      "num_agent_steps_sampled: 400\n",
      "num_agent_steps_trained: 400\n",
      "num_env_steps_sampled: 400\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 400\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf: {}\n",
      "pid: 7313\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0426742102058278\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.07766239625437603\n",
      "  mean_inference_ms: 0.5870486436929229\n",
      "  mean_raw_obs_processing_ms: 0.06491614843994137\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -200.0\n",
      "  episode_reward_mean: -200.0\n",
      "  episode_reward_min: -200.0\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -200.0\n",
      "    - -200.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0426742102058278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07766239625437603\n",
      "    mean_inference_ms: 0.5870486436929229\n",
      "    mean_raw_obs_processing_ms: 0.06491614843994137\n",
      "time_since_restore: 0.3333094120025635\n",
      "time_this_iter_s: 0.16356706619262695\n",
      "time_total_s: 0.3333094120025635\n",
      "timers:\n",
      "  learn_throughput: 23838.722\n",
      "  learn_time_ms: 8.39\n",
      "  load_throughput: 1029908.901\n",
      "  load_time_ms: 0.194\n",
      "  training_iteration_time_ms: 166.378\n",
      "  update_time_ms: 0.038\n",
      "timestamp: 1655607282\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "warmup_time: 0.3437197208404541\n",
      "\n",
      "agent_timesteps_total: 600\n",
      "counters:\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_env_steps_sampled: 600\n",
      "  num_env_steps_trained: 600\n",
      "custom_metrics: {}\n",
      "date: 2022-06-19_11-54-42\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -200.0\n",
      "episode_reward_mean: -200.0\n",
      "episode_reward_min: -200.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 3\n",
      "experiment_id: 4618d3c50d314674b7d4a8c2235cbbb5\n",
      "hostname: G990FXA8370E\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        policy_loss: -62.75297546386719\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 200.0\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_env_steps_sampled: 600\n",
      "  num_env_steps_trained: 600\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.20.53.128\n",
      "num_agent_steps_sampled: 600\n",
      "num_agent_steps_trained: 600\n",
      "num_env_steps_sampled: 600\n",
      "num_env_steps_sampled_this_iter: 200\n",
      "num_env_steps_trained: 600\n",
      "num_env_steps_trained_this_iter: 200\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf: {}\n",
      "pid: 7313\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.042617857872335574\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.07815939131932144\n",
      "  mean_inference_ms: 0.5856692348359406\n",
      "  mean_raw_obs_processing_ms: 0.06417681087889811\n",
      "sampler_results:\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -200.0\n",
      "  episode_reward_mean: -200.0\n",
      "  episode_reward_min: -200.0\n",
      "  episodes_this_iter: 1\n",
      "  hist_stats:\n",
      "    episode_lengths:\n",
      "    - 200\n",
      "    - 200\n",
      "    - 200\n",
      "    episode_reward:\n",
      "    - -200.0\n",
      "    - -200.0\n",
      "    - -200.0\n",
      "  off_policy_estimator: {}\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.042617857872335574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07815939131932144\n",
      "    mean_inference_ms: 0.5856692348359406\n",
      "    mean_raw_obs_processing_ms: 0.06417681087889811\n",
      "time_since_restore: 0.49211907386779785\n",
      "time_this_iter_s: 0.15880966186523438\n",
      "time_total_s: 0.49211907386779785\n",
      "timers:\n",
      "  learn_throughput: 27005.434\n",
      "  learn_time_ms: 7.406\n",
      "  load_throughput: 1081006.186\n",
      "  load_time_ms: 0.185\n",
      "  training_iteration_time_ms: 163.769\n",
      "  update_time_ms: 0.035\n",
      "timestamp: 1655607282\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "warmup_time: 0.3437197208404541\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "def collect_experience(num_episodes):\n",
    "    config[\"output\"] = \"./experience/\"\n",
    "    env = gym.make(config[\"env\"])\n",
    "    agent = pg.PGTrainer(config=config)\n",
    "    for i in range(num_episodes):\n",
    "        print(pretty_print(agent.train()))\n",
    "\n",
    "collect_experience(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 1,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 0.0001,\n",
       " 'train_batch_size': 256,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'recreate_failed_workers': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'eager_max_retraces': 20,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_duration': 10,\n",
       " 'evaluation_duration_unit': 'episodes',\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'always_attach_evaluation_results': False,\n",
       " 'keep_per_episode_custom_metrics': False,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'metrics_episode_collection_timeout_s': 180,\n",
       " 'metrics_num_episodes_for_smoothing': 100,\n",
       " 'min_time_s_per_reporting': 1,\n",
       " 'min_train_timesteps_per_reporting': 100,\n",
       " 'min_sample_timesteps_per_reporting': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': [],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_config': {},\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_map_capacity': 100,\n",
       "  'policy_map_cache': None,\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " '_tf_policy_handles_more_than_one_loss': False,\n",
       " '_disable_preprocessor_api': False,\n",
       " '_disable_action_flattening': False,\n",
       " '_disable_execution_plan_api': True,\n",
       " 'disable_env_checking': False,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'evaluation_num_episodes': -1,\n",
       " 'metrics_smoothing_episodes': -1,\n",
       " 'timesteps_per_iteration': -1,\n",
       " 'min_iter_time_s': -1,\n",
       " 'collect_metrics_timeout': -1,\n",
       " 'twin_q': True,\n",
       " 'use_state_preprocessor': -1,\n",
       " 'Q_model': {'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': None,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {}},\n",
       " 'policy_model': {'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': None,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {}},\n",
       " 'tau': 0.005,\n",
       " 'initial_alpha': 1.0,\n",
       " 'target_entropy': 'auto',\n",
       " 'n_step': 1,\n",
       " 'buffer_size': -1,\n",
       " 'replay_buffer_config': {'_enable_replay_buffer_api': False,\n",
       "  'type': 'MultiAgentReplayBuffer',\n",
       "  'capacity': 1000000},\n",
       " 'store_buffer_in_checkpoints': False,\n",
       " 'prioritized_replay': False,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'training_intensity': None,\n",
       " 'optimization': {'actor_learning_rate': 0.0003,\n",
       "  'critic_learning_rate': 0.0003,\n",
       "  'entropy_learning_rate': 0.0003},\n",
       " 'grad_clip': None,\n",
       " 'learning_starts': 1500,\n",
       " 'target_network_update_freq': 0,\n",
       " 'worker_side_prioritization': False,\n",
       " '_deterministic_loss': False,\n",
       " '_use_beta_distribution': False,\n",
       " 'bc_iters': 20000,\n",
       " 'temperature': 1.0,\n",
       " 'num_actions': 10,\n",
       " 'lagrangian': False,\n",
       " 'lagrangian_thresh': 5.0,\n",
       " 'min_q_weight': 5.0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = cql.CQL_DEFAULT_CONFIG.copy()\n",
    "config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('ml-bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee554be5372d57a1a8d0c8789d456036f96c86b5a6259585cc7b77cce32bbd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
