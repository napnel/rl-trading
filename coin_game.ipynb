{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Contribution by the Center on Long-Term Risk:\n",
    "# https://github.com/longtermrisk/marltoolbox\n",
    "##########\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo, dqn \n",
    "from ray.rllib.examples.env.coin_game_non_vectorized_env import CoinGame, AsymCoinGame\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--tf\", action=\"store_true\")\n",
    "parser.add_argument(\"--stop-iters\", type=int, default=2000)\n",
    "\n",
    "\n",
    "def main(debug, stop_iters=2000, tf=False, asymmetric_env=False):\n",
    "    train_n_replicates = 1 if debug else 1\n",
    "    seeds = list(range(train_n_replicates))\n",
    "\n",
    "    ray.init()\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": 2 if debug else stop_iters,\n",
    "    }\n",
    "\n",
    "    env_config = {\n",
    "        \"players_ids\": [\"player_red\", \"player_blue\"],\n",
    "        \"max_steps\": 20,\n",
    "        \"grid_size\": 3,\n",
    "        \"get_additional_info\": True,\n",
    "    }\n",
    "\n",
    "    rllib_config = {\n",
    "        \"env\": AsymCoinGame if asymmetric_env else CoinGame,\n",
    "        \"env_config\": env_config,\n",
    "        \"multiagent\": {\n",
    "            \"policies\": {\n",
    "                env_config[\"players_ids\"][0]: (\n",
    "                    None,\n",
    "                    AsymCoinGame(env_config).OBSERVATION_SPACE,\n",
    "                    AsymCoinGame.ACTION_SPACE,\n",
    "                    {},\n",
    "                ),\n",
    "                env_config[\"players_ids\"][1]: (\n",
    "                    None,\n",
    "                    AsymCoinGame(env_config).OBSERVATION_SPACE,\n",
    "                    AsymCoinGame.ACTION_SPACE,\n",
    "                    {},\n",
    "                ),\n",
    "            },\n",
    "            \"policy_mapping_fn\": lambda agent_id, **kwargs: agent_id,\n",
    "        },\n",
    "        # Size of batches collected from each worker.\n",
    "        \"rollout_fragment_length\": 20,\n",
    "        # Number of timesteps collected for each SGD round.\n",
    "        # This defines the size of each SGD epoch.\n",
    "        \"train_batch_size\": 512,\n",
    "        \"model\": {\n",
    "            \"dim\": env_config[\"grid_size\"],\n",
    "            \"conv_filters\": [\n",
    "                [16, [3, 3], 1],\n",
    "                [32, [3, 3], 1],\n",
    "            ],  # [Channel, [Kernel, Kernel], Stride]]\n",
    "        },\n",
    "        \"lr\": 5e-3,\n",
    "        \"seed\": tune.grid_search(seeds),\n",
    "        \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "        \"framework\": \"tf\" if tf else \"torch\",\n",
    "    }\n",
    "\n",
    "    tune_analysis = tune.run(\n",
    "        PPO,\n",
    "        config=rllib_config,\n",
    "        stop=stop,\n",
    "        checkpoint_freq=0,\n",
    "        checkpoint_at_end=True,\n",
    "        name=\"PPO_AsymCG\",\n",
    "    )\n",
    "    ray.shutdown()\n",
    "    return tune_analysis\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    debug_mode = True\n",
    "    use_asymmetric_env = False\n",
    "    main(debug_mode, args.stop_iters, args.tf, use_asymmetric_env)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
