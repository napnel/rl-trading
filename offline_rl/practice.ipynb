{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ray.rllib.agents import cql, sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"Pendulum-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = sac.DEFAULT_CONFIG.copy()\n",
    "# config[\"env\"] = env\n",
    "# config[\"output\"] = \"./experience\"\n",
    "# sampling_agent = sac.SACTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# while True:\n",
    "#     print(f\"Iter {i}\")\n",
    "#     results = sampling_agent.train()\n",
    "#     reward = results.get(\"episode_reward_mean\")\n",
    "\n",
    "#     if reward >= -500:\n",
    "#         break\n",
    "\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 05:51:17,453\tWARNING deprecation.py:45 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
      "2022-01-24 05:51:17,480\tWARNING json_reader.py:63 -- Treating input directory as glob patterns: ['/home/yoshiakira/ml-bot/offline_rl/experience/*.json', '/home/yoshiakira/ml-bot/offline_rl/experience/*.zip']\n",
      "2022-01-24 05:51:22,421\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16200 batches (16200 ts) into the replay buffer, which has capacity 1000000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=6667)\u001b[0m 2022-01-24 05:51:28,074\tWARNING deprecation.py:45 -- DeprecationWarning: `convert_to_non_torch_type` has been deprecated. Use `ray/rllib/utils/numpy.py::convert_to_numpy` instead. This will raise an error in the future!\n",
      "2022-01-24 05:51:28,101\tINFO trainable.py:124 -- Trainable.setup took 11.002 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0\n",
      "... R=nan\n",
      "... R=-1071.500645140501\n",
      "Iter 1\n",
      "... R=nan\n",
      "... R=-1325.5430553778046\n",
      "Iter 2\n",
      "... R=nan\n",
      "... R=-1359.2794287110291\n",
      "Iter 3\n",
      "... R=nan\n",
      "... R=-1158.6344841593325\n",
      "Iter 4\n",
      "... R=nan\n",
      "... R=-1201.5456201004852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0124 06:40:03.755474800   32405 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1642974003.755433100\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1642974003.755427900\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "E0124 09:20:23.795829100   20773 backup_poller.cc:134]       Run client channel backup poller: {\"created\":\"@1642983623.795787300\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1642983623.795783200\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n"
     ]
    }
   ],
   "source": [
    "config = cql.CQL_DEFAULT_CONFIG.copy()\n",
    "config[\"num_workers\"] = 0  # Run locally.\n",
    "config[\"horizon\"] = 200\n",
    "config[\"soft_horizon\"] = True\n",
    "config[\"no_done_at_end\"] = True\n",
    "config[\"n_step\"] = 3\n",
    "config[\"bc_iters\"] = 20000\n",
    "config[\"clip_actions\"] = False\n",
    "config[\"normalize_actions\"] = True\n",
    "config[\"learning_starts\"] = 256\n",
    "config[\"rollout_fragment_length\"] = 1\n",
    "config[\"prioritized_replay\"] = False\n",
    "config[\"tau\"] = 0.005\n",
    "config[\"target_entropy\"] = \"auto\"\n",
    "config[\"Q_model\"] = {\n",
    "    \"fcnet_hiddens\": [256, 256],\n",
    "    \"fcnet_activation\": \"relu\",\n",
    "}\n",
    "config[\"policy_model\"] = {\n",
    "    \"fcnet_hiddens\": [256, 256],\n",
    "    \"fcnet_activation\": \"relu\",\n",
    "}\n",
    "config[\"optimization\"] = {\n",
    "    \"actor_learning_rate\": 3e-4,\n",
    "    \"critic_learning_rate\": 3e-4,\n",
    "    \"entropy_learning_rate\": 3e-4,\n",
    "}\n",
    "config[\"train_batch_size\"] = 256\n",
    "config[\"target_network_update_freq\"] = 1\n",
    "config[\"timesteps_per_iteration\"] = 1000\n",
    "data_file = \"./experience\"\n",
    "# print(\"data_file={} exists={}\".format(data_file, os.path.isfile(data_file)))\n",
    "config[\"input\"] = data_file\n",
    "# config[\"log_level\"] = \"INFO\"\n",
    "config[\"env\"] = \"Pendulum-v1\"\n",
    "\n",
    "# Set up evaluation.\n",
    "config[\"evaluation_num_workers\"] = 1\n",
    "config[\"evaluation_interval\"] = 1\n",
    "config[\"evaluation_duration\"] = 10\n",
    "# This should be False b/c iterations are very long and this would\n",
    "# cause evaluation to lag one iter behind training.\n",
    "config[\"evaluation_parallel_to_training\"] = False\n",
    "# Evaluate on actual environment.\n",
    "config[\"evaluation_config\"] = {\"input\": \"sampler\"}\n",
    "config[\"framework\"] = \"torch\"\n",
    "\n",
    "# Check, whether we can learn from the given file in `num_iterations`\n",
    "# iterations, up to a reward of `min_reward`.\n",
    "num_iterations = 5\n",
    "min_reward = -300\n",
    "\n",
    "# Test for torch framework (tf not implemented yet).\n",
    "trainer = cql.CQLTrainer(config=config)\n",
    "learnt = False\n",
    "for i in range(num_iterations):\n",
    "    print(f\"Iter {i}\")\n",
    "    results = trainer.train()\n",
    "    eval_results = results.get(\"evaluation\")\n",
    "    print(\"... R={}\".format(results[\"episode_reward_mean\"]))\n",
    "    print(\"... R={}\".format(eval_results[\"episode_reward_mean\"]))\n",
    "    # Learn until some reward is reached on an actual live env.\n",
    "    if results[\"episode_reward_mean\"] >= min_reward:\n",
    "        learnt = True\n",
    "        break\n",
    "# if not learnt:\n",
    "#     raise ValueError(\n",
    "#         \"CQLTrainer did not reach {} reward from expert \"\n",
    "#         \"offline data!\".format(min_reward)\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60ed309031a8768d278d7c7b1359a9e37f49534c66ee819e22d1da4dbbcd2a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('ml-bot': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
